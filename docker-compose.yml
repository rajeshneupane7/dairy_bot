version: '3.8'

services:
  # Smart Dairy AI Application
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: smart-dairy-ai
    ports:
      - "3000:3000"
    environment:
      # Database
      - DATABASE_URL=file:./dev.db

      # Node environment
      - NODE_ENV=production

      # Ollama configuration (when running Ollama in separate container)
      - OLLAMA_BASE_URL=http://ollama:11434

      # For host Ollama, use: - OLLAMA_BASE_URL=http://host.docker.internal:11434

      # Model configuration
      - OLLAMA_MODEL=llama3
      - LLM_MODEL=llama3

      # z-ai-web-dev-sdk configuration
      - ZAI_BASE_URL=${ZAI_BASE_URL:-http://ollama:11434}
      - ZAI_MODEL=${ZAI_MODEL:-llama3}

    volumes:
      # Persist uploads
      - ./uploads:/app/uploads
      # Persist database
      - ./db:/app/db
      # Persist Prisma migrations
      - ./prisma:/app/prisma

    depends_on:
      - ollama

    restart: unless-stopped

    networks:
      - dairy-ai-network

    # Health check
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Ollama Service (Local LLM)
  ollama:
    image: ollama/ollama:latest
    container_name: smart-dairy-ollama
    ports:
      - "11434:11434"
    volumes:
      # Persist Ollama models
      - ollama-models:/root/.ollama

    # GPU support (uncomment if you have NVIDIA GPU with nvidia-docker)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*

    restart: unless-stopped

    command: >
      sh -c "
        if [ ! -f /app/db/dev.db ]; then
          echo 'Database not found, initializing...';
          npm run db:push;
        fi
        npm start
      "

    networks:
      - dairy-ai-network

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # Optional: Adminer for database management (if using PostgreSQL in production)
  # adminer:
  #   image: adminer:latest
  #   container_name: smart-dairy-adminer
  #   ports:
  #     - "8080:8080"
  #   networks:
  #     - dairy-ai-network
  #   restart: unless-stopped

# Named volumes
volumes:
  ollama-models:
    driver: local

# Networks
networks:
  dairy-ai-network:
    driver: bridge
